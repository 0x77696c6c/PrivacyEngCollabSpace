# Differentially Private Stochastic Gradient Descent (DP-SGD) 

**Brief Description:** Train machine learning models with differential privacy by clipping and noising gradients during stochastic gradient descent. 

**Link to Tool:** [https://github.com/tensorflow/models/tree/master/research/differential_privacy/dp_sgd](https://github.com/tensorflow/models/tree/master/research/differential_privacy/dp_sgd) 

**Primary Tool Focus Area:** De-identification 

**GitHub User Serving as POC:** @ilyamironov 

**Additional Notes:** Paper with full details: [https://arxiv.org/abs/1607.00133]()